**6장. 키-값 저장소 설계 (Key-Value Store)**

---

## 1. 키-값 저장소 개요

* 키

  * 문자열, 숫자, 해시된 값 등
* 값

  * 문자열, JSON, 리스트, 직렬화된 객체 등
* 기본 연산

  * put(key, value)
  * get(key)

---

## 2. 문제 이해 및 설계 범위

설계 대상 키-값 저장소의 요구사항을 먼저 정리한다.

* 키-값 기준

  * 키-값 쌍 크기: 10KB 이하
  * 하지만, 큰 데이터도 저장 가능해야 함 (큰 값은 별도 스토리지에 두고 포인터만 저장하는 식으로 설계 가능)
* 비기능 요구

  * 높은 가용성
  * 높은 규모 확장성

    * 트래픽 양에 따라 자동으로 샤딩/노드 증설·감축
  * 데이터 일관성 수준 조정 가능

    * 강한 일관성 vs 결과적 일관성 등
  * 낮은 지연 시간

    * 밀리초 단위 응답

---

## 3. 단일 서버 키-값 저장소

* 단일 서버 버전은 상대적으로 단순하다.

  * 인메모리 맵 + 디스크 파일 저장
  * 간단한 인덱스, 해시맵 등
* 단점

  * 단일 장애점(SPOF)
  * 용량 한계, 성능 병목
* 보완 아이디어(책에서 언급)

  * 데이터 압축
  * 자주 쓰는 데이터만 메모리에 두고, 나머지 데이터는 디스크에 저장

    * LRU 캐시 + 파일 시스템 조합

실제 우리가 관심 있는 것은 “분산 키-값 저장소”이므로, 나머지는 분산 관점으로 확장한다.

---

## 4. 분산 키-값 저장소 개념

### 4.1 분산 해시 테이블(DHT)

* 여러 노드에 키-값을 분산 저장하기 위한 구조
* 키를 해시하여 각 노드에 매핑
* 노드 수가 커져도 일정한 규칙으로 키를 찾아갈 수 있는 구조
* 안정 해시(Consistent Hashing)를 활용해

  * 노드 추가/삭제 시 데이터 이동량을 최소화

---

## 5. CAP 정리

* Consistency (일관성)

  * 어떤 노드에 접속하든 항상 같은 데이터를 본다.
* Availability (가용성)

  * 일부 노드에 장애가 나도, 항상 응답을 돌려준다.
* Partition Tolerance (파티션 감내)

  * 네트워크가 갈라져도(노드 간 통신 장애) 시스템은 계속 동작해야 한다.

분산 시스템에서 네트워크 파티션은 언젠가 반드시 발생한다고 가정하므로,
현실적으로는 “CP vs AP” 중에 선택해야 하는 상황이 자주 등장한다.

### 5.1 CP 시스템 (Consistency + Partition Tolerance)

* 특징

  * 네트워크 파티션이 발생하면 “일관성”을 우선
  * 일관성을 깨뜨릴 수 있는 요청(특히 쓰기)은 거부하여, 가용성을 희생
* 예시

  * HBase

    * Zookeeper를 통해 리전 서버 관리
    * 마스터/리전 서버 간 통신이 깨지면, 쓰기를 멈추고 재선출 과정 진행
  * Zookeeper 자체

    * 과반수(quorum)를 확보하지 못하면 쓰기/읽기 일부 중단
* 구체적 동작 이미지

  * 노드 n1, n2, n3가 있을 때, n3와 나머지 간 파티션 발생
  * CP 선택 시

    * 일관성 깨질 위험이 있으면 n1, n2에서 쓰기 중단 (또는 일부 요청 거부)
    * 대신 “읽을 때는 항상 같은 값”을 보장

### 5.2 AP 시스템 (Availability + Partition Tolerance)

* 특징

  * 네트워크 파티션이 나도 서비스 중단을 최소화
  * 각 파티션에서 쓰기를 계속 받고, 나중에 동기화
  * 일시적으로 서로 다른 버전의 데이터를 볼 수 있음 → 결과적 일관성
* 예시

  * Amazon Dynamo 계열

    * Dynamo, DynamoDB 설계 철학
  * Cassandra
  * Riak
* “대부분 AP 시스템을 쓰고, 일관성은 나중에 보정한다”라는 생각

  * 맞는 방향이다.
  * AP 계열 시스템은

    * 클라이언트가 최신/과거 데이터를 볼 수 있는 위험을 감수하는 대신
    * 항상 응답을 주고
    * 읽기·쓰기 후에 “read repair / anti-entropy / conflict resolution”로 정합성을 맞춰가는 구조를 많이 사용한다.

### 5.3 CA 시스템

* 파티션 감내를 고려하지 않는 시스템
* 이론적으로만 존재, 현실 분산 환경에서는 네트워크 파티션을 피할 수 없으므로 거의 의미 없다.

---

## 6. 시스템 컴포넌트 설계

### 6.1 데이터 파티션

* 목적

  * 데이터를 여러 노드에 나눠서 저장
* 고려사항

  * 여러 서버에 고르게 분산할 수 있는가?
  * 노드 추가·삭제 시 데이터 이동을 최소화할 수 있는가?
* 해결책

  * 해시 링 + 안정 해시

    * 5장에서 정리한 것처럼

      * 키를 링 위에 해시
      * 시계 방향 첫 서버가 담당
      * 서버 추가/삭제 시 일부 키만 이동

---

### 6.2 데이터 다중화 (Replication)

질문: 여기서 말하는 데이터 다중화가 마스터 DB를 다중화하는 건가요? 아니면 복제를 하는 건가요?

* 여기서 말하는 “다중화”는

  * 특정 키-값을 여러 서버에 “여러 사본(replica)”으로 저장하는 것을 의미한다.
  * 즉, “마스터 DB를 여러 개 둔다”가 아니라

    * 하나의 논리적 데이터(키)에 대해 N개의 사본을 서로 다른 노드에 복제하는 것.
* 예시

  * N = 3

    * 어떤 키 K는 노드 A, B, C에 동시에 저장
    * A 장애 발생 시 B나 C에서 읽고/쓰기를 처리
* 목적

  * 높은 가용성
  * 데이터 내구성(노드 일부가 망가져도 데이터 보존)

---

### 6.3 정족수 합의(Quorum)와 일관성

* N: 각 데이터의 사본 개수
* W: 쓰기 성공으로 인정할 최소 응답 개수
* R: 읽기 성공으로 인정할 최소 응답 개수

조합에 따라 특성이 달라진다.

* R = 1, W = N

  * 쓰기 시 모든 노드에 반드시 반영 → 쓰기 느림
  * 읽기는 아무 노드나 1개만 성공해도 되므로 빠름
* W = 1, R = N

  * 쓰기는 한 노드만 성공해도 되니 빠름
  * 읽기는 모든 노드에서 확인해야 해서 느림

강한 일관성 조건:

* W + R > N

  * 최소 하나의 노드가 “읽기와 쓰기 쿼럼에 동시에 포함”됨
  * 따라서 최신 쓰기가 반영된 노드를 적어도 하나 읽게 된다.
* 반대로 W + R ≤ N 이면

  * 강한 일관성을 보장하지 못하고, 결과적으로 eventually consistent 가 된다.

일관성 모델 요약:

* 강한 일관성

  * 항상 최신 데이터를 본다.
* 약한 일관성

  * 언제 최신이 될지 보장하지 않는다.
* 결과적 일관성

  * 어느 시점 이후에는 결국 모든 사본이 같은 값으로 수렴
  * Dynamo, Cassandra 같은 시스템은 기본적으로 결과적 일관성을 사용한다.

---

### 6.4 일관성 불일치 해소

여러 노드에 같은 키의 서로 다른 버전이 생길 수 있다. 이를 어떻게 해결할까?

#### 데이터 버전닝 + 벡터 시계(Vector Clock)

* 버전닝

  * 업데이트마다 데이터의 버전 정보를 함께 관리
  * 단순하게는 (버전 번호 1,2,3…)로 관리할 수 있지만,
  * 분산 환경에서는 노드별로 독립적인 변경이 일어나 충돌이 생길 수 있다.

* 벡터 시계란?

  * [서버, 버전] 쌍을 여러 개 모아 둔 구조

    * 예: {A:3, B:1, C:0}
  * 각 노드(A,B,C)가 해당 데이터에 대해 몇 번 변경했는지 기록

* 구체 예시

  * 초기: {A:1}
  * A가 다시 수정: {A:2}
  * B가 A의 버전 {A:2}를 보고 수정: {A:2, B:1}
  * 동시에 C가 {A:2} 버전을 기반으로 수정: {A:2, C:1}
  * 나중에 읽어보니

    * {A:2, B:1} 과 {A:2, C:1} 이라는 두 버전이 존재
  * 두 버전은 서로 “이전/이후 관계”가 아니므로 충돌 상태

    * 애플리케이션이 머지 로직을 구현하거나,
    * 비즈니스 규칙에 따라 어떤 버전을 선택해야 함

* 이전 버전 판단

  * 한 벡터 시계 X의 모든 카운터가 Y의 카운터 이하이면, X는 Y의 이전 버전
  * 그렇지 않고 서로 교차되면(어떤 값은 크고 어떤 값은 작으면) 충돌

---

### 6.5 장애 처리

#### 장애 감지

* 멀티캐스트 방식

  * 모든 노드가 서로 상태를 브로드캐스트하는 방식
  * 노드 수가 많아지면 비효율적

* 가십 프로토콜(Gossip Protocol)

  * 각 노드가 일부 노드와만 주기적으로 상태 정보를 교환
  * 하트비트 카운터를 포함해 “누가 살아 있는지” 정보를 퍼뜨림
  * 시간이 지나면 클러스터 전체가 거의 같은 “노드 상태”를 공유하게 됨

#### 일시적 장애 처리 (Hinted Handoff)

* 어떤 노드가 잠시 다운된 경우

  * 원래 그 노드가 받아야 할 쓰기를, 다른 노드가 “임시로 대신 저장”
  * 나중에 장애 노드가 복구되면

    * 임시로 저장해둔 노드가 해당 변경 내용을 건네주고 동기화

#### 영구 장애 처리 (Anti-Entropy, Merkle Tree)

* 오랫동안 동기화가 안 된 노드들 간 데이터 맞추기
* 머클 트리(Merkle Tree)

  * 각 노드의 데이터(키 밸류 세트)를 트리 구조로 해시
  * 루트 해시를 먼저 비교

    * 같으면 전체 데이터가 동일
    * 다르면, 아래 레벨로 내려가면서 어느 서브트리(버킷)에서 차이가 나는지 찾아감
  * 결국 차이가 나는 버킷만 동기화하면 되므로 효율적

---

## 7. 시스템 아키텍처, 읽기/쓰기 경로

### 7.1 쓰기 경로 (LSM-Tree 스타일)

1. 클라이언트 쓰기 요청
2. 먼저 커밋 로그(Write-Ahead Log)에 기록

   * 장애 시 데이터 복구용
3. 메모리 캐시(예: memtable)에 데이터 기록
4. 메모리 캐시가 임계치에 도달하면

   * 디스크에 SSTable 파일 형태로 flush
   * 백그라운드에서 여러 SSTable을 머지/컴팩션

### 7.2 읽기 경로

1. 메모리 캐시에서 해당 키를 먼저 조회
2. 없다면 블룸 필터 확인

   * 이 키가 특정 디스크 파일에 존재할 가능성이 있는지 확인
   * “없다”고 나오면 그 파일은 스킵
3. 블룸 필터가 “있을 수 있다”고 하면 디스크에서 해당 SSTable 탐색
4. 찾은 데이터를 반환

   * 필요 시 메모리 캐시에 올려서 이후 조회를 빠르게 처리

---

## 8. 마무리 요약

* 키-값 저장소는 단순한 인터페이스(put/get)를 가지지만,

  * 분산, 샤딩, 복제, 일관성, 장애 처리까지 고려하면 매우 복잡한 시스템이다.
* 핵심 키워드

  * 분산 해시 테이블, 안정 해시
  * CAP 정리(CP vs AP)와 일관성 모델
  * 복제(다중화), 정족수 합의(N, R, W)
  * 버전닝, 벡터 시계, read repair
  * 가십 프로토콜, hinted handoff, 머클 트리