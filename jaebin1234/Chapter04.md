# 4장. 처리율 제한 장치의 설계 (Rate Limiter)

## 1. 처리율 제한 장치란?

* 클라이언트 또는 서비스가 보내는 트래픽의 처리율을 제어하는 컴포넌트
* 정의한 임계치 이상으로 API 요청이 들어오면, 추가 요청을 거부하거나 지연시킴
* 대표 예시

  * 사용자는 초당 2회 이상 새 글을 올릴 수 없다
  * 같은 IP 주소로는 하루에 10개 이상의 계정을 생성할 수 없다
  * 같은 디바이스로는 주당 5회 이상 리워드를 요청할 수 없다
* 목적

  * DoS 공격, 실수에 의한 과도한 호출 방지
  * 백엔드 자원(DB, 캐시, 외부 API 등)의 고갈 방지
  * 공정한 자원 사용 보장
  * 예: 트위터(3시간 300개 트윗), 구글 독스(사용자당 분당 300회 read 제한 등)

---

## 2. 1단계: 문제 이해 및 설계 범위 확정

면접/설계 시작 시, 아래 질문들로 요구사항을 명확히 한다.

### 2.1 어떤 종류의 처리율 제한 장치인가?

* 어디에 둘 것인가?

  * 클라이언트 단에서 제한할 것인가?
  * 서버(애플리케이션 코드)에서 제한할 것인가?
  * 미들웨어(API 게이트웨이, 프록시)에서 제한할 것인가?
* 어떤 기준으로 제한할 것인가?

  * API 엔드포인트별
  * 사용자 ID별
  * IP 주소별
  * 디바이스 ID별
  * 복합 키(사용자 ID + API, IP + API 등)

### 2.2 시스템 특성 및 범위

* 예상 트래픽 규모는 어느 정도인가?
* 시스템이 분산 환경(다중 서버, 다중 인스턴스)에서 동작하는가?
* 처리율 제한 장치는

  * 독립된 서비스인가?
  * 각 애플리케이션 서버 내 라이브러리/코드 형태인가?

### 2.3 사용자 경험 관련 요구

* 요청이 제한되었을 때

  * 429 Too Many Requests 응답을 줄 것인가?
  * 언제 다시 시도할 수 있는지 알려줄 것인가? (Retry-After 헤더 등)
  * 일부 요청은 큐에 적재 후 지연 처리할 것인가?

### 2.4 비기능 요구사항

* 처리율 초과 요청을 정확하게 제한할 것
* 낮은 응답 시간 (rate limiter 자체가 병목이 되면 안 됨)
* 가능한 적은 메모리 사용
* 분산 환경에서도 정확한 처리율 제한
* 예외 처리(실패 시에도 전체 서비스가 죽지 않도록)
* 높은 결함 감내성(일부 노드 장애 시에도 시스템 전체는 동작)

---

## 3. 2단계: 개략적 설계안 제시

### 3.1 배치 위치 선택

대표적인 세 위치:

* 클라이언트

  * 장점: 서버까지 도달하는 트래픽 자체를 줄일 수 있음
  * 단점: 신뢰할 수 없음(클라이언트 조작 가능)
* 애플리케이션 서버

  * 장점: 구현이 직관적, 비즈니스 로직과 함께 처리 가능
  * 단점: 서버 수가 늘어나면 분산 카운팅이 어려워짐
* 미들웨어(API 게이트웨이, 프록시)

  * 장점: 여러 서비스 앞단에서 공통 정책으로 적용 가능
  * 단점: 게이트웨이가 병목이 될 수 있으며, 별도 운영이 필요

실무/면접에서는 보통
“MSA 환경에서 API 게이트웨이(Nginx, Kong, Spring Cloud Gateway 등)에 rate limiter를 두고,
내부적으로 Redis를 이용해 카운터를 관리하는 구조”를 많이 이야기한다.

### 3.2 기본 동작 흐름

1. 클라이언트가 API 호출
2. 요청이 API 게이트웨이(또는 미들웨어)의 처리율 제한 필터를 통과
3. 처리율 제한 알고리즘(토큰 버킷 등)이 현재 상태를 확인

   * 허용 가능: 요청을 백엔드 서비스로 전달
   * 허용 불가: 429 Too Many Requests 응답
4. 응답 시, X-RateLimit-* 헤더로 현재 남은 쿼터, 윈도 정보 등을 내려줄 수 있음

---

## 4. 처리율 제한 알고리즘 비교

### 4.1 토큰 버킷(Token Bucket)

* 개념

  * 일정 속도로 토큰이 들어오는 버킷이 있고, 요청 1개당 토큰 1개를 소비
  * 버킷이 가득 차면 추가 토큰은 버림
  * 토큰이 없으면 요청을 거부(또는 대기)

* 동작

  * 버킷 크기: 최대 버스트 트래픽 허용량
  * 토큰 공급률: 단위 시간당 허용 처리율
  * API 요청 하나 처리 시 토큰 1개 사용

* 장점

  * 구현이 비교적 쉽고, 메모리 효율적
  * 짧은 시간 동안의 버스트 트래픽도 허용하면서, 평균 처리율은 제어 가능

* 단점

  * 버킷 크기와 토큰 공급률 두 파라미터를 튜닝하기가 까다로움

---

### 4.2 누출 버킷(Leaky Bucket)

* 개념

  * FIFO 큐로 구현
  * 일정 속도로 큐에서 요청을 꺼내 처리 (고정된 처리율)

* 동작

  * 새 요청 도착

    * 큐에 빈 자리 있으면 enqueue
    * 큐가 가득 차 있으면 새 요청은 버림
  * 지정된 시간마다 큐에서 요청을 dequeue하여 처리

* 장점

  * 큐 크기가 제한되어 있어서 메모리 사용량 예측 가능
  * 출력 처리율이 일정하여 백엔드에 안정된 부하 제공

* 단점

  * 짧은 시간에 트래픽이 몰리면 최신 요청이 계속 버려질 수 있음
  * 버킷 크기와 처리율 튜닝이 어렵다

---

### 4.3 고정 윈도 카운터(Fixed Window Counter)

* 개념

  * 타임라인을 동일한 길이의 윈도(예: 1분)로 나누고, 윈도별 카운터를 둔다.
  * 각 요청 도착 시 해당 윈도의 카운터 +1
  * 카운터가 임계치를 넘으면 새 윈도가 열릴 때까지 요청 거부

* 장점

  * 구현과 이해가 쉽고, 메모리 효율이 좋다
  * 특정 패턴(예: 분당 N회 제한)에 적합

* 단점

  * 윈도 경계 문제

    * 예: 00:00:59에 100회, 00:01:01에 100회 요청이 들어오면
      “2초 동안 200회”가 허용되어, 기대보다 많은 요청 처리 가능

---

### 4.4 이동 윈도 로그(Sliding Window Log)

* 개념

  * 고정 윈도 카운터의 경계 문제를 해결하기 위해 도입
  * 요청마다 타임스탬프를 기록(예: Redis sorted set 사용)
  * 일정 윈도(예: 직전 1분) 내의 요청 수만 세어 제한

* 동작

  * 새 요청이 왔을 때

    * 현재 윈도 시작 시점보다 오래된 타임스탬프 삭제
    * 새 타임스탬프 추가
    * 현재 로그 길이가 허용치 이하면 통과, 아니면 거부

* 장점

  * 윈도 경계를 자연스럽게 슬라이딩하면서, 정확하게 처리율 한도 유지

* 단점

  * 허용/거부된 모든 요청의 타임스탬프를 보관해야 해서 메모리를 많이 사용할 수 있음

---

### 4.5 이동 윈도 카운터(Sliding Window Counter)

* 개념

  * 고정 윈도 카운터 + 이동 윈도 로그의 절충안
  * 정확한 로그 대신, 직전 윈도의 카운트를 이용해 현재 윈도의 요청 수를 근사

* 동작

  * 현재 1분 윈도의 카운트 +
    직전 1분 윈도의 카운트 × (현재 시간과 직전 윈도가 겹치는 비율)
  * 이 추정치가 한도보다 작으면 허용, 크면 거부

* 장점

  * 고정 윈도의 메모리 효율을 유지하면서, 경계 문제를 완화
  * 짧은 시간에 몰리는 트래픽에도 어느 정도 잘 대응

* 단점

  * 근사치를 사용하므로, 엄밀히 보면 약간 느슨한 한도
  * 대부분의 서비스에서는 이 정도 오차는 큰 문제가 되지 않음

---

### 4.6 알고리즘 요약 표

| 알고리즘      | 메커니즘               | 장점                   | 단점                   |
| --------- | ------------------ | -------------------- | -------------------- |
| 토큰 버킷     | 버킷에 토큰을 채우고 소모     | 버스트 허용, 구현·메모리 효율 좋음 | 파라미터(크기, 공급률) 튜닝 어려움 |
| 누출 버킷     | FIFO 큐에서 고정 속도로 누출 | 일정한 처리율, 메모리 사용 예측   | 최신 요청이 버려질 수 있음      |
| 고정 윈도 카운터 | 윈도마다 카운터 증가        | 단순, 메모리 효율           | 윈도 경계 트래픽 문제         |
| 이동 윈도 로그  | 타임스탬프 로그를 모두 보관    | 정확한 한도 유지            | 메모리 사용량 큼            |
| 이동 윈도 카운터 | 윈도별 카운터 + 비율 기반 추정 | 메모리 효율 + 경계 문제 완화    | 근사치 기반이라 약간 느슨함      |

---

## 5. 개략 아키텍처

### 5.1 기준 키 선택

* 사용자별

  * key 예: rate:user:{userId}
* IP 주소별

  * key 예: rate:ip:{ip}
* API 엔드포인트별

  * key 예: rate:api:{path}
* 복합 키

  * key 예: rate:user:{userId}:api:{path}

요구사항에 따라
“사용자별 + API 별” 같이 조합 가능.

### 5.2 카운터 저장 위치

* Redis 사용 (가장 일반적인 선택)

  * 단일 스레드, 명령 단위 원자성 보장
  * INCR, EXPIRE 등을 활용
  * Lua 스크립트를 사용해 INCR + EXPIRE 같은 연산을 원자적으로 처리 가능
* 간단한 경우, 애플리케이션 메모리(단일 서버)에서도 가능하지만

  * 서버가 여러 대일 경우 분산 카운팅 문제 발생
  * 재시작 시 카운터 유실

---

## 6. 상세 설계 포인트

### 6.1 처리율 제한 규칙 예시

* 마케팅 메시지: 사용자당 하루 최대 5개
* 로그인 시도: 클라이언트별로 분당 최대 5회
* 비밀번호 재설정 이메일: 이메일 주소당 시간당 3회

요구사항을 규칙으로 정형화해서
rate limiter에 설정 가능한 정책으로 만든다.

### 6.2 처리율 초과 트래픽 처리

* 기본은 HTTP 429 Too Many Requests 응답
* HTTP 헤더 예시

  * X-RateLimit-Remaining: 현재 윈도에서 남은 요청 수
  * X-RateLimit-Limit: 윈도 내 최대 요청 수
  * X-RateLimit-Retry-After: 다시 시도할 수 있는 시간(초/타임스탬프)
* 일부 비즈니스에서는

  * 메시지 큐에 적재 후 천천히 처리하는 전략도 사용 가능

### 6.3 분산 환경 이슈

* 경쟁 조건

  * 여러 게이트웨이 인스턴스가 동시에 동일한 키를 갱신
  * Redis 단일 명령(INCR, INCRBY)은 원자적이지만

    * “읽기 → 비교 → 쓰기” 패턴은 경쟁 조건 발생 가능
    * Lua 스크립트로 한 번에 처리하거나, 슬라이딩 윈도 구현 시 주의
* 동기화

  * 카운터를 중앙 저장소(주로 Redis)에 몰아주어 일관성 확보
  * 로컬 캐시만 사용하면 인스턴스별로 다른 한도가 적용될 수 있음

### 6.4 성능 최적화

* Redis 연결 풀 및 타임아웃 적절히 설정
* 스크립트/Lua 사용으로 네트워크 라운드트립 최소화
* 너무 세밀한 윈도(예: 100ms 단위)는 제어 정확도는 높지만 오버헤드 증가

### 6.5 모니터링

* 제한된 요청 수, 429 응답 수
* 사용자·IP·API별 상위 사용량
* 알람

  * 특정 API에서 갑자기 429가 급증하면 공격 또는 버그 가능성

---

## 7. 마무리 요약

* 처리율 제한 장치는 트래픽 폭주, DoS, 자원 고갈을 막기 위한 핵심 컴포넌트다.
* 설계 시에는

  * 어디에 둘지(API 게이트웨이, 서버, 클라이언트)
  * 어떤 기준 키(사용자, IP, API)를 쓸지
  * 어떤 알고리즘(토큰 버킷, 이동 윈도 등)을 쓸지
  * 분산 환경에서의 카운터 일관성, 성능, 모니터링
    를 단계적으로 정리해야 한다.
* 실무에서는

  * API 게이트웨이 + Redis + 토큰 버킷/슬라이딩 윈도 카운터 조합이 가장 많이 쓰이는 패턴이다.
